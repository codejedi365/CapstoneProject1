{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:24:15.164410Z",
     "start_time": "2019-08-24T16:24:15.117536Z"
    }
   },
   "source": [
    "In this section of the notebook, I will be exploring the data and answering the following questions:\n",
    "\n",
    "   1. Is there something intereseting to count?\n",
    "   2. Are there any trends (e.g. high, low, increasing, decreasing, anomalies)?\n",
    "   3. Are there any valuable comparisons between two related quantities?\n",
    "  \n",
    "I used histograms, bar plots, scatterplots, and time-series plots to answer the following questions:\n",
    "\n",
    "   4. Are there any insights from the data?\n",
    "   5. Are there any correlations? \n",
    "   6. What is a hypothesis that can be taken further?\n",
    "   7. What other questions arise from these insights and correlations?\n",
    "   \n",
    "After answering these questions, I provide a link to a presentation that uses text and plots to tell the compelling story of my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T13:02:00.116162Z",
     "start_time": "2019-09-01T13:01:58.450427Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "import visualization as vz\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textwrap import wrap\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file = '../data/manipulated_data/rainfalldata.csv'\n",
    "rd = pd.read_csv(file)\n",
    "file2 = '../data/manipulated_data/ncrainfalldata.csv'\n",
    "ncrd = pd.read_csv(file2)\n",
    "rd.Date = pd.to_datetime(rd.Date)\n",
    "rd = rd.set_index('Date')\n",
    "ncrd.Date = pd.to_datetime(ncrd.Date)\n",
    "ncrd = ncrd.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:38:17.751498Z",
     "start_time": "2019-08-24T16:37:54.422Z"
    }
   },
   "outputs": [],
   "source": [
    "rd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:38:17.759495Z",
     "start_time": "2019-08-24T16:38:01.065Z"
    }
   },
   "outputs": [],
   "source": [
    "ncrd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the average rainfall per month for Raleigh, NC as a bar graph using the standard error of the mean for error bars. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthavg = []\n",
    "monthsem = []\n",
    "monthstd = []\n",
    "for i in range(1,13):\n",
    "    monthavg.append(np.mean(rd['Raleigh, NC'][rd.index.month == i]))\n",
    "    monthsem.append(stats.sem(rd['Raleigh, NC'][rd.index.month == i]))\n",
    "    monthstd.append(np.std(rd['Raleigh, NC'][rd.index.month == i]))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(rd.index.month.unique(), monthavg, yerr = monthsem, alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_title('Average Monthly Rainfall in Raleigh, NC from 1956 to 2019')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Rainfall (in)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('raleighmonthly.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the data as average rainfall per year and comparing two different sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdyearavg = rd.resample('Y').mean()\n",
    "rdyearavg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearlyavgfigs(df, loc, **keyword_parameters):\n",
    "    ''' this function takes in a dataframe and a location (column) and displays the average yearly rainfall \n",
    "    for each location. Only can take up to 2 locations\n",
    "    '''\n",
    "    plt.figure(figsize = (400/96, 400/96),dpi=96)\n",
    "    if len(loc) == 1:\n",
    "        if ('color' in keyword_parameters):\n",
    "            plt.plot(df.index.year, df[loc[0]], keyword_parameters['color'])\n",
    "        else:\n",
    "            plt.plot(df.index.year, df[loc[0]])\n",
    "        plt.title('Average Yearly Rainfall in ' + loc[0] + ' from 1980 to 2019')\n",
    "    else:\n",
    "        plt.plot(df.index.year, df[loc[0]])\n",
    "        plt.plot(df.index.year, df[loc[1]])\n",
    "        plt.title(\"\\n\".join(wrap('Average Yearly Rainfall in ' + loc[0] + '-' + loc[1] + ' from 1980 to 2019', \n",
    "                                 60)))\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Rainfall (in)')\n",
    "    plt.show()\n",
    "yearlyavgfigs(rdyearavg, ['Raleigh, NC'])\n",
    "yearlyavgfigs(rdyearavg, ['Greensboro AP, NC'], color='orange')\n",
    "yearlyavgfigs(rdyearavg, ['Raleigh, NC', 'Greensboro AP, NC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a seasonality to rainfall amounts throughout the year. The following steps utilize seasonal decomposition to investigate the how often the seasonality occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T17:21:33.569832Z",
     "start_time": "2019-08-24T17:21:33.445903Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "plt.rcParams[\"font.size\"] = 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(rd['Raleigh, NC'], freq=12, model='multiplicative')\n",
    "result.plot()\n",
    "plt.savefig('seasonalityral.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It appears that there may be a slight increase towards the end of the sample towards an increase in rainfall amounts. Therefore, the following cells looks into whether there is a positive trend in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = result.trend\n",
    "t2col = t.reset_index()\n",
    "t2col = t2col.dropna()\n",
    "t2col = t2col.reset_index()\n",
    "x = np.array(t2col.index).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(t)\n",
    "tdf = tdf.dropna()\n",
    "y = np.array(tdf['Raleigh, NC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As shown by the Least Squares model below. The best fit line for the trend data is a 0 degree line with one coefficient. Basically showing that there is no positive or negative correlation in rainfall over the past 40 years in Raleigh, NC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at the correlations between only 25 locations. \"rd.iloc\" can be manipulated to be any other set of 25 locations to see the correlations between those locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nc heatmap\n",
    "rd_fr25 = rd.iloc[:,0:25]\n",
    "vz.get_corr_heat_map(rd_fr25, ignore_cancelled = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "Beginning with autocorrelation of all target (NC) locations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    rs = int(result.size/2)\n",
    "    return result[rs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr(rd['Raleigh, NC'])\n",
    "plot_acf(rd['Raleigh, NC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There appear to be correlation peaks located at every prior 12 months. Showing that the previous year's rainfall for that month is correlated to that month's rainfall. Next looks at the correlation between the rainfall of the current month to the rainfall of the previous month (lag 1) and the rainfall of the same month from the prior year (lag 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_corr(df,lag=1):\n",
    "    df2 = df.copy()\n",
    "    cols = df2.columns\n",
    "    for col in df2.columns:\n",
    "        df2[col+'_'+str(lag)] = df2[col].shift(lag)\n",
    "    df2=df2.dropna()\n",
    "    correlation = df2.corr()\n",
    "    correlation = correlation.drop(cols, axis=1)\n",
    "    correlation = correlation.iloc[0:len(cols)]\n",
    "    return(correlation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_corr1 = lag_corr(ncrd)\n",
    "nc_corr1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag12ncrd = lag_corr(ncrd,lag=12)\n",
    "lag12ncrd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must run Data Wrangling report first and save the dictionary from there. At the very bottom of the report it \n",
    "# stores the exogenous locations as a dictionary. The target location is the key with the exogenous locations \n",
    "# as a list of strings of the exogenous locations name. \n",
    "%store -r exogen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below shows one location with its exogenous variables as the lag 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exolag(df, location, lag=1):\n",
    "    df2 = df.copy()\n",
    "    lt = exogen[location]\n",
    "    lt2 = lt.copy()\n",
    "    lt2.append(location)\n",
    "    locdf = df2[lt2]\n",
    "    exol = lag_corr(locdf, lag=lag)\n",
    "    return(exol)\n",
    "exolag(rd, 'Albemarle, NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exolag(rd, 'Albemarle, NC', lag=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarima Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - fitting the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_model_creation(data, p, d, q, P, D, Q, m, exog=None):\n",
    "    my_order = [p,d,q]\n",
    "    my_sorder = [P,D,Q,m]\n",
    "    sarimamod = sm.tsa.statespace.SARIMAX(data, exog, order=my_order, seasonal_order=my_sorder, \n",
    "                                          enforce_stationarity=False, enforce_invertibility=False,\n",
    "                                          initialization='approximate_diffuse')\n",
    "    model_fit = sarimamod.fit()# start_params=[0, 0, 0, 0, 1])\n",
    "    return(model_fit)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - separating the training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = rd['Raleigh, NC'].iloc[0:376]\n",
    "validation = rd['Raleigh, NC'].iloc[376:424]\n",
    "# used to train the model during the test of the never before seen test data\n",
    "test_training = rd['Raleigh, NC'].iloc[0:424] \n",
    "testing = rd['Raleigh, NC'].iloc[424:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Creating a baseline forecast, without the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = sarima_model_creation(training, 0,0,0,0,0,0,12)\n",
    "baseline.forecast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Finding the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_hyper(it):\n",
    "    ''' This function takes a number and creates a list of lists that each contain a number from zero to the \n",
    "    provided number (it)\n",
    "    '''\n",
    "    outlist = []\n",
    "    for AR in range(it):\n",
    "        for MA in range(it):\n",
    "            for SAR in range(it):\n",
    "                for SMA in range(it):\n",
    "                    outlist.append([AR,MA,SAR,SMA])\n",
    "    return(outlist)\n",
    "        \n",
    "config = iteration_hyper(5) # creates all possible numbers for each parameter from 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_find(training_data, comb, testing_data):\n",
    "    ''' this function uses the training data and testing data to find out which combination of hyperparameters\n",
    "    best predicts the following months rainfall.\n",
    "    '''\n",
    "    leastmae = 1000\n",
    "    for com in comb:\n",
    "        li_one_step = []\n",
    "        for i in range(len(testing_data)): # iterate through the testing data\n",
    "            if i is not 0:\n",
    "                # create a model from all the data that includes the addition of the actual rainfall amount\n",
    "                # from the previous month\n",
    "                mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, com[3], 12)\n",
    "                one_step_pred = mod_1.forecast() # make the prediction for the next month\n",
    "                li_one_step.append(one_step_pred[0]) # save prediction to a list\n",
    "                copytraining = pd.concat([copytraining, testing_data[[i]]]) # add the true rainfall value\n",
    "            else:\n",
    "                copytraining = training_data.copy() # make a copy of the dataset\n",
    "                mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, com[3], 12)\n",
    "                one_step_pred2 = mod_1.forecast()\n",
    "                li_one_step.append(one_step_pred2[0])\n",
    "                copytraining = pd.concat([copytraining, testing_data[[i]]])\n",
    "        # find the mean absolute error between the what the rainfall was and what the model predicted it to be        \n",
    "        mae = mean_absolute_error(testing_data, li_one_step) \n",
    "        if mae < leastmae:\n",
    "            leastmae = mae\n",
    "            H_AR = com[0]\n",
    "            H_MA = com[1]\n",
    "            H_SAR = com[2]\n",
    "            H_SMA = com[3]\n",
    "        print(com,mae) # due to the length of time this function takes to run, this provides an update of each\n",
    "        # combination and the Mean Absolute error for that model run with the given parameters\n",
    "    return('AR: '+ str(H_AR), 'MA: ' +str(H_MA), 'SAR: '+str(H_SAR), 'SMA: '+str(H_SMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a very long time to run as there are 625 different possiblities. DO NOT UN-Comment and run all\n",
    "# hyperparameter_find(training, config, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters for this data set were: p=4, d=0, q=3, P=3, D=0, Q=4, m=12\n",
    "\n",
    "    p: Trend autoregression order.\n",
    "    d: Trend difference order.\n",
    "    q: Trend moving average order.\n",
    "    P: Seasonal autoregressive order.\n",
    "    D: Seasonal difference order.\n",
    "    Q: Seasonal moving average order.\n",
    "    m: The number of time steps for a single seasonal period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing model on never before seen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb = [[4,3,3,4]]\n",
    "hyperparameter_find(test_training, best_comb, testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error for training data = 1.23234368392907 Mean Absolute Error for Testing data = 1.8150609730404315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Determining if any exogenous (external) locations outside of NC will increase the performace of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
