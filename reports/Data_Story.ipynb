{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Story\n",
    "\n",
    "This jupyter notebook explains the entire process that took place when analyzing the rainfall data and predicting rainfall data for the next 50 years. It first begins with exploratory data analysis, then moves to creating a Sarima model, then finishes by predicting the next 50 years of monthly rainfall from the sarima model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:24:15.164410Z",
     "start_time": "2019-08-24T16:24:15.117536Z"
    }
   },
   "source": [
    "In this section of the notebook, I will be exploring the data and answering the following questions:\n",
    "\n",
    "   1. Is there something intereseting to count?\n",
    "   2. Are there any trends (e.g. high, low, increasing, decreasing, anomalies)?\n",
    "   3. Are there any valuable comparisons between two related quantities?\n",
    "  \n",
    "I used histograms, bar plots, scatterplots, and time-series plots to answer the following questions:\n",
    "\n",
    "   4. Are there any insights from the data?\n",
    "   5. Are there any correlations? \n",
    "   6. What is a hypothesis that can be taken further?\n",
    "   7. What other questions arise from these insights and correlations?\n",
    "   \n",
    "After answering these questions, I provide a link to a presentation that uses text and plots to tell the compelling story of my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T13:02:00.116162Z",
     "start_time": "2019-09-01T13:01:58.450427Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "import visualization as vz\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textwrap import wrap\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from collections import defaultdict\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file = '../data/manipulated_data/rainfalldata.csv'\n",
    "rd = pd.read_csv(file)\n",
    "file2 = '../data/manipulated_data/ncrainfalldata.csv'\n",
    "ncrd = pd.read_csv(file2)\n",
    "rd.Date = pd.to_datetime(rd.Date)\n",
    "rd = rd.set_index('Date')\n",
    "ncrd.Date = pd.to_datetime(ncrd.Date)\n",
    "ncrd = ncrd.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:38:17.751498Z",
     "start_time": "2019-08-24T16:37:54.422Z"
    }
   },
   "outputs": [],
   "source": [
    "rd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T16:38:17.759495Z",
     "start_time": "2019-08-24T16:38:01.065Z"
    }
   },
   "outputs": [],
   "source": [
    "ncrd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the average rainfall per month for Raleigh, NC as a bar graph using the standard error of the mean for error bars. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthavg = []\n",
    "monthsem = []\n",
    "monthstd = []\n",
    "for i in range(1,13):\n",
    "    monthavg.append(np.mean(rd['Raleigh, NC'][rd.index.month == i]))\n",
    "    monthsem.append(stats.sem(rd['Raleigh, NC'][rd.index.month == i]))\n",
    "    monthstd.append(np.std(rd['Raleigh, NC'][rd.index.month == i]))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(rd.index.month.unique(), monthavg, yerr = monthsem, alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_title('Average Monthly Rainfall in Raleigh, NC from 1956 to 2019')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Rainfall (in)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('raleighmonthly.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the data as average rainfall per year and comparing two different sites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdyearavg = rd.resample('Y').mean()\n",
    "rdyearavg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearlyavgfigs(df, loc, **keyword_parameters):\n",
    "    ''' this function takes in a dataframe and a location (column) and displays the average yearly rainfall \n",
    "    for each location. Only can take up to 2 locations\n",
    "    '''\n",
    "    plt.figure(figsize = (400/96, 400/96),dpi=96)\n",
    "    if len(loc) == 1:\n",
    "        if ('color' in keyword_parameters):\n",
    "            plt.plot(df.index.year, df[loc[0]], keyword_parameters['color'])\n",
    "        else:\n",
    "            plt.plot(df.index.year, df[loc[0]])\n",
    "        plt.title('Average Yearly Rainfall in ' + loc[0] + ' from 1980 to 2019')\n",
    "    else:\n",
    "        plt.plot(df.index.year, df[loc[0]])\n",
    "        plt.plot(df.index.year, df[loc[1]])\n",
    "        plt.title(\"\\n\".join(wrap('Average Yearly Rainfall in ' + loc[0] + '-' + loc[1] + ' from 1980 to 2019', \n",
    "                                 60)))\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Rainfall (in)')\n",
    "    plt.show()\n",
    "yearlyavgfigs(rdyearavg, ['Raleigh, NC'])\n",
    "yearlyavgfigs(rdyearavg, ['Greensboro AP, NC'], color='orange')\n",
    "yearlyavgfigs(rdyearavg, ['Raleigh, NC', 'Greensboro AP, NC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a seasonality to rainfall amounts throughout the year. The following steps utilize seasonal decomposition to investigate the how often the seasonality occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-24T17:21:33.569832Z",
     "start_time": "2019-08-24T17:21:33.445903Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "plt.rcParams[\"font.size\"] = 32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = seasonal_decompose(rd['Raleigh, NC'], freq=12, model='multiplicative')\n",
    "result.plot()\n",
    "plt.savefig('seasonalityral.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It appears that there may be a slight increase towards the end of the sample towards an increase in rainfall amounts. Therefore, the following cells looks into whether there is a positive trend in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = result.trend\n",
    "t2col = t.reset_index()\n",
    "t2col = t2col.dropna()\n",
    "t2col = t2col.reset_index()\n",
    "x = np.array(t2col.index).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(t)\n",
    "tdf = tdf.dropna()\n",
    "y = np.array(tdf['Raleigh, NC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As shown by the Least Squares model below. The best fit line for the trend data is a 0 degree line with one coefficient. Basically showing that there is no positive or negative correlation in rainfall over the past 40 years in Raleigh, NC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, x).fit()\n",
    "predictions = model.predict(x) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at the correlations between only 25 locations. \"rd.iloc\" can be manipulated to be any other set of 25 locations to see the correlations between those locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nc heatmap\n",
    "rd_fr25 = rd.iloc[:,0:25]\n",
    "vz.get_corr_heat_map(rd_fr25, ignore_cancelled = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "\n",
    "Beginning with autocorrelation of all target (NC) locations only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    rs = int(result.size/2)\n",
    "    return result[rs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr(rd['Raleigh, NC'])\n",
    "plot_acf(rd['Raleigh, NC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There appear to be correlation peaks located at every prior 12 months. Showing that the previous year's rainfall for that month is correlated to that month's rainfall. Next looks at the correlation between the rainfall of the current month to the rainfall of the previous month (lag 1) and the rainfall of the same month from the prior year (lag 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_corr(df,lag=1):\n",
    "    df2 = df.copy()\n",
    "    cols = df2.columns\n",
    "    for col in df2.columns:\n",
    "        df2[col+'_'+str(lag)] = df2[col].shift(lag)\n",
    "    df2=df2.dropna()\n",
    "    correlation = df2.corr()\n",
    "    correlation = correlation.drop(cols, axis=1)\n",
    "    correlation = correlation.iloc[0:len(cols)]\n",
    "    return(correlation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_corr1 = lag_corr(ncrd)\n",
    "nc_corr1.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag12ncrd = lag_corr(ncrd,lag=12)\n",
    "lag12ncrd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must run Data Wrangling report first and save the dictionary from there. At the very bottom of the report it \n",
    "# stores the exogenous locations as a dictionary. The target location is the key with the exogenous locations \n",
    "# as a list of strings of the exogenous locations name. \n",
    "%store -r exogen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below shows one location with its exogenous variables as the lag 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exolag(df, location, lag=1):\n",
    "    df2 = df.copy()\n",
    "    lt = exogen[location]\n",
    "    lt2 = lt.copy()\n",
    "    lt2.append(location)\n",
    "    locdf = df2[lt2]\n",
    "    exol = lag_corr(locdf, lag=lag)\n",
    "    return(exol)\n",
    "exolag(rd, 'Albemarle, NC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exolag(rd, 'Albemarle, NC', lag=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarima Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - fitting the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_model_creation(data, p, d, q, P, D, Q, m, exog=None):\n",
    "    my_order = [p,d,q]\n",
    "    my_sorder = [P,D,Q,m]\n",
    "    sarimamod = sm.tsa.statespace.SARIMAX(data, exog, order=my_order, seasonal_order=my_sorder, \n",
    "                                          enforce_stationarity=False, enforce_invertibility=False,\n",
    "                                          initialization='approximate_diffuse')\n",
    "    model_fit = sarimamod.fit()# start_params=[0, 0, 0, 0, 1])\n",
    "    return(model_fit)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - separating the training, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = rd['Raleigh, NC'].iloc[0:376]\n",
    "validation = rd['Raleigh, NC'].iloc[376:424]\n",
    "# used to train the model during the test of the never before seen test data\n",
    "test_training = rd['Raleigh, NC'].iloc[0:424] \n",
    "testing = rd['Raleigh, NC'].iloc[424:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Creating a baseline forecast, without the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = sarima_model_creation(training, 0,0,0,0,0,0,12)\n",
    "baseline.forecast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Finding the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_hyper(it):\n",
    "    ''' This function takes a number and creates a list of lists that each contain a number from zero to the \n",
    "    provided number (it)\n",
    "    '''\n",
    "    outlist = []\n",
    "    for AR in range(it):\n",
    "        for MA in range(it):\n",
    "            for SAR in range(it):\n",
    "                for SMA in range(it):\n",
    "                    outlist.append([AR,MA,SAR,SMA])\n",
    "    return(outlist)\n",
    "        \n",
    "config = iteration_hyper(5) # creates all possible numbers for each parameter from 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_find(training_data, comb, testing_data):\n",
    "    ''' this function uses the training data and testing data to find out which combination of hyperparameters\n",
    "    best predicts the following months rainfall.\n",
    "    '''\n",
    "    leastmae = 1000\n",
    "    for com in comb:\n",
    "        li_one_step = []\n",
    "        for i in range(len(testing_data)): # iterate through the testing data\n",
    "            if i is not 0:\n",
    "                # create a model from all the data that includes the addition of the actual rainfall amount\n",
    "                # from the previous month\n",
    "                mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, com[3], 12)\n",
    "                one_step_pred = mod_1.forecast() # make the prediction for the next month\n",
    "                li_one_step.append(one_step_pred[0]) # save prediction to a list\n",
    "                copytraining = pd.concat([copytraining, testing_data[[i]]]) # add the true rainfall value\n",
    "            else:\n",
    "                copytraining = training_data.copy() # make a copy of the dataset\n",
    "                mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, com[3], 12)\n",
    "                one_step_pred2 = mod_1.forecast()\n",
    "                li_one_step.append(one_step_pred2[0])\n",
    "                copytraining = pd.concat([copytraining, testing_data[[i]]])\n",
    "        # find the mean absolute error between the what the rainfall was and what the model predicted it to be        \n",
    "        mae = mean_absolute_error(testing_data, li_one_step) \n",
    "        if mae < leastmae:\n",
    "            leastmae = mae\n",
    "            H_AR = com[0]\n",
    "            H_MA = com[1]\n",
    "            H_SAR = com[2]\n",
    "            H_SMA = com[3]\n",
    "        print(com,mae) # due to the length of time this function takes to run, this provides an update of each\n",
    "        # combination and the Mean Absolute error for that model run with the given parameters\n",
    "    return('AR: '+ str(H_AR), 'MA: ' +str(H_MA), 'SAR: '+str(H_SAR), 'SMA: '+str(H_SMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a very long time to run as there are 625 different possiblities. DO NOT UN-Comment and run all\n",
    "# hyperparameter_find(training, config, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters for this data set were: p=4, d=0, q=3, P=3, D=0, Q=4, m=12\n",
    "\n",
    "    p: Trend autoregression order.\n",
    "    d: Trend difference order.\n",
    "    q: Trend moving average order.\n",
    "    P: Seasonal autoregressive order.\n",
    "    D: Seasonal difference order.\n",
    "    Q: Seasonal moving average order.\n",
    "    m: The number of time steps for a single seasonal period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing model on never before seen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb = [[4,3,3,4]]\n",
    "hyperparameter_find(test_training, best_comb, testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Absolute Error for training data = 1.23234368392907 Mean Absolute Error for Testing data = 1.8150609730404315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Determining if any exogenous (external) locations outside of NC will increase the performace of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creation_pred_one_step(train_data, test_data, exotrain=None, exotest=None):\n",
    "    list_one_step = []\n",
    "\n",
    "    mod = sarima_model_creation(train_data, 4, 0, 3, 3, 0, 4, 12, exotrain)\n",
    "    # if exists, passing exotrain's prevMonth (december, for forecasting jan), otherwise only forcast based on \n",
    "    # model\n",
    "    nextMonth = mod.forecast() if exotrain is None else mod.forecast( exotrain.iloc[[-1]] )# turnary assignment expression\n",
    "    list_one_step.append(nextMonth[0]) # captures prediction\n",
    "\n",
    "    # if test data exists\n",
    "    if len(test_data) >= 1:\n",
    "        # increment data for next month's iteration\n",
    "        train_data = pd.concat([train_data, test_data[[0]]])\n",
    "        test_data = test_data.drop(test_data.index[0], axis = 0)\n",
    "        if exotrain is not None:\n",
    "            exotrain = pd.concat([exotrain, exotest[[0]]])\n",
    "            exotest = exotest.drop(exotest.index[0], axis = 0)\n",
    "\n",
    "        # execute & capture future predictions\n",
    "        futurePredictions = model_creation_pred_one_step(train_data, test_data, exotrain, exotest)\n",
    "        # add to list\n",
    "        list_one_step.append(futurePredictions)\n",
    "    \n",
    "    return(list_one_step)\n",
    "\n",
    "# previously billsFn\n",
    "def maeFinder(train_data, test_data, exotrain=None, exotest=None):\n",
    "    clone_train_data = train_data.copy()\n",
    "    clone_test_data = test_data.copy()\n",
    "    clone_exotrain = exotrain if exotrain is None else exotrain.copy()\n",
    "    clone_exotest = exotest if exotest is None else exotest.copy()\n",
    "\n",
    "    predictions = model_creation_pred_one_step(clone_train_data, clone_test_data, clone_exotrain, clone_exotest)\n",
    "    mae = mean_absolute_error(testing_data, predictions)\n",
    "\treturn(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exog_combinations(df, exoe):\n",
    "    ''' This function takes the dataframe of rain data and the list of exogenous variables from a single NC\n",
    "    location and then returns a list of dataframes that contains all of the rainfall data for just the \n",
    "    exogenous variables\n",
    "    '''\n",
    "    lo_dfs = []\n",
    "    if len(exoe) == 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "    if len(exoe) > 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "        for ex in exoe:\n",
    "            lo_dfs.append(df.loc[:,[ex]])\n",
    "        if len(exoe) >2:\n",
    "            for i in range(2, len(exoe)):\n",
    "                combolist = list(combinations(exoe,i))\n",
    "                for c in combolist:\n",
    "                    lo_dfs.append(df.loc[:,c])\n",
    "    return(lo_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous function, it is placed into a for loop which creates a full list of dataframes for every location and exogenous location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T14:49:13.135489Z",
     "start_time": "2019-09-01T14:49:12.698005Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'defaultdict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8c78b5151f29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml_o_dfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_exogen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlo_dfs2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexog_combinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0ml_o_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlo_dfs2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ml_o_dfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'LONGWOOD, NC'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"
     ]
    }
   ],
   "source": [
    "l_o_dfs = defaultdict(list)\n",
    "for key,value in tqdm(exogen.items()):\n",
    "    lo_dfs2 = exog_combinations(rd, value)\n",
    "    l_o_dfs[key] = lo_dfs2\n",
    "l_o_dfs['LONGWOOD, NC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exogenous_var(data, ncloc, l_exoloc):\n",
    "    #     for key, value in tqdm(exo_dict.items()):\n",
    "    dat = data[ncloc]\n",
    "    #         l_exog = exog_combinations(data, value)\n",
    "    tr, test = train_test_split(dat, 0.2, False)\n",
    "    keymae = maeFinder(tr, test)\n",
    "    print('keymae of: '+ key +' = '+str(keymae))\n",
    "    bettermae = {}\n",
    "    bettermaeLock = Lock()\n",
    "\n",
    "    def find_exmae(exog, l):\n",
    "        extr, extest = train_test_split(exog, 0.2, False)\n",
    "        exmae = maeFinder(tr, test, extr, extest)\n",
    "        co = tuple(exog.columns)\n",
    "        if result[\"exmae\"] < keymae:\n",
    "            l.acquire()\n",
    "            try:\n",
    "                bettermae[co] = exmae\n",
    "                bettermae2 = {key: bettermae}\n",
    "            finally:\n",
    "                l.release()\n",
    "\n",
    "        return { \"co\": co, \"exmae\": exmae }\n",
    "\n",
    "    def on_success(result):\n",
    "        print('exmae = {}'.format(result[\"co\"]) + ' '+ str(result[\"exmae\"]))\n",
    "\n",
    "    def on_error():\n",
    "        # do something\n",
    "\n",
    "\n",
    "    process_limit = multiprocessing.cpu_count()-1\n",
    "    pool = multiprocessing.Semaphore(process_limit)\n",
    "    # num_exmaes = len(list(l_exoloc.keys()))\n",
    "    for exog in l_exoloc:\n",
    "        pool.apply_async(find_exmae, (exog, bettermaeLock), None, on_success, on_error)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following for loop goes through all of the NC locations and creates a model and finds the performance of\n",
    "the model using Mean Absolute Error. It then goes through includes each of the possible combinations of the \n",
    "location's exogenous variables and creates a model and determines the performance of the model. Thus, it makes a model for all 46 locations with exogenous locations that are within 50 kilometers. Then it creates a model for one of the 46 target locations including their exogenous locations. If a target location has one exogenous location, then it will create one model for the target location and one for the target location with the exogenous variable included. However, if the target location has 2 or more exogenous locations it will still create a model for the target and a model for the target with each exogenous variable included. However, it will also include a forth model that creates a model with the target, target and both exogenous locations. Greater than 2 exogenous locations will also include every possible combination of the exogenous locations as separate models. All models are evaluated by Mean Absolute Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:13:39.500982Z",
     "start_time": "2019-09-01T15:13:39.469726Z"
    }
   },
   "outputs": [],
   "source": [
    "# for key,value in tqdm(l_o_dfs.items()):\n",
    "#     exogenous_var(rd, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 8 of the 46 locations performed better including the exogenous variables, and are listed below: \n",
    "'WHITEVILLE 7 NW, NC', 'CASAR, NC', 'FOREST CITY 8 W, NC', 'GASTONIA, NC', 'LAKE LURE 2, NC', \n",
    "                       'ELIZABETHTOWN, NC', ' MOUNT HOLLY 4 NE, NC','GRANDFATHER MTN, NC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 8 locations had exogenous variables that increased the performance of the model; therefore, a majority of the locations could be run for predictions only including the locations' rainfall data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removal of locations that have exogenous variables\n",
    "with_exogs = ['WHITEVILLE 7 NW, NC', 'CASAR, NC', 'FOREST CITY 8 W, NC', 'GASTONIA, NC', 'LAKE LURE 2, NC', \n",
    "                       'ELIZABETHTOWN, NC', ' MOUNT HOLLY 4 NE, NC','GRANDFATHER MTN, NC']\n",
    "ncrd2 = ncrd.copy()\n",
    "ncrd_less = ncrd2.drop(with_exogs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fx(data, begin, end):\n",
    "    ''' this function uses the dataframe without exogenous variables and creates a model, fits the model, and \n",
    "    then predicts the next 50 years of rainfall data as both a point prediction and a confidence interval\n",
    "    '''\n",
    "    base = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    date_list = [base + relativedelta(months=x) for x in range(600)]\n",
    "    prediction1_df = pd.DataFrame(index=date_list)\n",
    "    for col in tqdm(data.columns):\n",
    "        loc = data[col]\n",
    "        mod_fit1 = sarima_model_creation(loc, 4,0,3,3,0,4,12)\n",
    "        point_predictions = pd.DataFrame(mod_fit1.predict(start=begin, end=end), columns=[col])\n",
    "        future_pred1 = mod_fit1.get_prediction(start=begin, end=end)\n",
    "        future_pred1_ci = future_pred1.conf_int(alpha=0.5)\n",
    "        point_predictions_df = pd.merge(point_predictions, future_pred1_ci, left_index=True, right_index=True)\n",
    "        prediction1_df = pd.merge(prediction1_df, point_predictions_df, left_index=True, right_index=True)\n",
    "    return(prediction1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = prediction_fx(ncrd_less, '2019-05-01', '2069-05-01')\n",
    "pre_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_var_dict2 = {\n",
    "    'WHITEVILLE 7 NW, NC': rd[[' LORIS 2 S, SC']],\n",
    "    'CASAR, NC': rd[['GAFFNEY 6 E, SC']],\n",
    "    'FOREST CITY 8 W, NC': rd[['GAFFNEY 6 E, SC']],\n",
    "    'GASTONIA, NC': rd[['FORT MILL 4 NW, SC','GAFFNEY 6 E, SC']],\n",
    "    'LAKE LURE 2, NC': rd[['CHESNEE 7 WSW, SC']],\n",
    "    ' MOUNT HOLLY 4 NE, NC': rd[['CHESNEE 7 WSW, SC','GAFFNEY 6 E, SC']],\n",
    "    'ELIZABETHTOWN, NC': rd[[' LORIS 2 S, SC']],\n",
    "    'GRANDFATHER MTN, NC': rd[['ELIZABETHTON, TN']]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_exog_fx2(data, exog_dict, begin, end):\n",
    "        ''' this function uses the dataframe out exogenous variables and creates a model, fits the model, and \n",
    "    then predicts the next 50 years of rainfall data as both a point prediction and a confidence interval\n",
    "    '''\n",
    "    base = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    date_list = [base + relativedelta(months=x) for x in range(600)]\n",
    "    prediction_df = pd.DataFrame(index = date_list)\n",
    "    pred_val_df = pd.DataFrame(index = date_list)\n",
    "    exog_predictions_df = pd.DataFrame(index = date_list)\n",
    "    for key,value in tqdm(exog_dict.items()):\n",
    "        loc = data[key]\n",
    "        mod_fit1 = sarima_model_creation(loc, 4,0,3,3,0,4, 12,exog=value)\n",
    "        if value.shape[1] > 1:\n",
    "            shap = value.shape[1]\n",
    "            for i in range(shap):\n",
    "                exog_mod_fit = sarima_model_creation(value.iloc[:,i],4,0,3,3,0,4,12)\n",
    "                e_preds2 = pd.DataFrame(exog_mod_fit.predict(start=begin, end=end))\n",
    "                if i is 0:\n",
    "                    exog_predictions_df = e_preds2\n",
    "                else:\n",
    "                    exog_predictions_df = pd.merge(exog_predictions_df, e_preds2, left_index=True, \n",
    "                                                   right_index=True)\n",
    "        else:\n",
    "            exog_mod_fit = sarima_model_creation(value, 4,0,3,3,0,4,12)\n",
    "            exog_predictions_df = pd.DataFrame(exog_mod_fit.predict(start=begin, end=end))\n",
    "        future_pred = mod_fit1.get_prediction(exog=exog_predictions_df,start=begin, end=end)\n",
    "        future_pred_ci = future_pred.conf_int(alpha=0.5)\n",
    "        future_pred_val= pd.DataFrame(mod_fit1.predict(exog=exog_predictions_df, start=begin, end=end), \n",
    "                                      columns = [key])\n",
    "        future_pred_full = pd.merge(future_pred_val, future_pred_ci, left_index=True, right_index=True)\n",
    "        prediction_df = pd.merge(prediction_df, future_pred_full, left_index=True, right_index=True)\n",
    "    return(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_ci_df = prediction_exog_fx2(rd, exo_var_dict2, '2019-05-01', '2069-05-01')\n",
    "e_ci_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals = pd.merge(pre_df, e_ci_df, left_index=True, right_index=True)\n",
    "merged_ci_vals.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals.to_csv('predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
