{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:51:04.069356Z",
     "start_time": "2019-09-01T15:49:22.228823Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import copy\n",
    "import multiprocessing\n",
    "import traceback\n",
    "import hashlib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try: \n",
    "    __file__\n",
    "except:\n",
    "    curr_dir = os.path.abspath('')\n",
    "else:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    \n",
    "app_root = curr_dir if os.path.basename(curr_dir) != \"src\" else os.path.dirname(curr_dir)\n",
    "\n",
    "if getpass.getuser() == \"rainfalld\":  # docker daemon\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    destdir = home                    # /var/cache/rainfall-predictor\n",
    "else:\n",
    "    destdir = os.path.join(app_root,'data','manipulated_data')      # non-docker stay in repository\n",
    "\n",
    "\n",
    "file = os.path.join(destdir,'rainfalldata.csv')\n",
    "rd = pd.read_csv(file)\n",
    "file2 = os.path.join(destdir,'ncrainfalldata.csv')\n",
    "ncrd = pd.read_csv(file2)\n",
    "rd.Date = pd.to_datetime(rd.Date)\n",
    "rd = rd.set_index('Date')\n",
    "ncrd.Date = pd.to_datetime(ncrd.Date)\n",
    "ncrd = ncrd.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T10:30:04.299825Z",
     "start_time": "2019-08-20T10:30:04.206119Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# this cell takes the stored exogen dictionary that is stored in the Data_Wrangling_CAP1 jupyter notebook\n",
    "# that was imported above.\n",
    "try:\n",
    "    %store -r exogen\n",
    "except NameError:\n",
    "    f = open(os.path.join(destdir,\"exogen.json\"),\"r\")\n",
    "    exogen = json.load(f)      # read from file, passed from Data_Wrangling\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Library\n",
    "Handles parallel-processing model calculation, mean absolute error calculations, and near-real-time calculation storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T15:51:04.428724Z",
     "start_time": "2019-09-01T15:51:04.413112Z"
    }
   },
   "outputs": [],
   "source": [
    "def sarima_model_creation(data, p, d, q, P, D, Q, m, exog=None):\n",
    "    my_order = [p,d,q]\n",
    "    my_sorder = [P,D,Q,m]\n",
    "    sarimamod = sm.tsa.statespace.SARIMAX(data, exog, order=my_order, seasonal_order=my_sorder, \n",
    "                                          enforce_stationarity=False, enforce_invertibility=False,\n",
    "                                          initialization='approximate_diffuse')\n",
    "    model_fit = sarimamod.fit(disp=0)   # start_params=[0, 0, 0, 0, 1])\n",
    "    return(model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T13:02:33.081089Z",
     "start_time": "2019-09-01T13:02:33.034217Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_creation_pred_one_step(train_data, test_data, exotrain=None, exotest=None, progress_bar=None):\n",
    "    ''' recursively makes forecast based on provided data for the next month\n",
    "        args: train_data = large data set to base predictions on\n",
    "              test_data  = decreasing dataset of data to test model\n",
    "              exotrain   = exogenous location data that matches the same timeframe of train_data but was not included\n",
    "              exotest    = exogenous location data that matches the same timeframe of test_data but was not included\n",
    "        returns: A list of all predictions for the location matching the entire test_data timeframe\n",
    "    '''\n",
    "    list_one_step = []\n",
    "    \n",
    "    nextMonth = model_based_forecast(train_data, exotrain)\n",
    "    list_one_step.append(nextMonth[0])             # captures prediction\n",
    "    progress_bar.update()\n",
    "\n",
    "    # if test data exists\n",
    "    if len(test_data) > 1:\n",
    "        # increment data for next month's iteration\n",
    "        train_data = pd.concat([train_data, test_data[[0]]])\n",
    "        test_data = test_data.drop(test_data.index[0], axis = 0)\n",
    "        if exotrain is not None:\n",
    "            exotrain = pd.concat([exotrain, exotest.iloc[0]])\n",
    "            exotest = exotest.drop(exotest.index[0], axis = 0)\n",
    "\n",
    "        # execute & capture future predictions\n",
    "        futurePredictions = model_creation_pred_one_step(train_data, test_data, exotrain, exotest, progress_bar)\n",
    "        # add to list\n",
    "        list_one_step.extend(futurePredictions)\n",
    "        \n",
    "    return(list_one_step)\n",
    "\n",
    "def model_based_forecast(train_data, exotrain=None):\n",
    "    ''' creates model from training data & makes a forecast\n",
    "        args: train_data = DataFrame to build forecasting model\n",
    "              exotrain   = DataFrame of exogenous location's rainfall data\n",
    "        returns: FLOAT value of next month's forecast value\n",
    "    '''\n",
    "    mod = sarima_model_creation(train_data, 4, 0, 3, 3, 0, 4, 12, exotrain)\n",
    "    # if exists, passing exotrain's prevMonth (december, for forecasting jan), otherwise only forcast based on model\n",
    "    nextMonth = mod.forecast() if exotrain is None else mod.forecast( exotrain.iloc[[-1]] )       # turnary assignment expression\n",
    "    return(nextMonth)\n",
    "\n",
    "def maeFinder(train_data, test_data, exotrain=None, exotest=None, pbar=None):\n",
    "    ''' Function that finds the Mean Absolute Error between test_data and model-based predictions\n",
    "        args: train_data = large data set to base predictions on\n",
    "              test_data  = decreasing dataset of data to test model\n",
    "              exotrain   = exogenous location data that matches the same timeframe of train_data but was not included\n",
    "              exotest    = exogenous location data that matches the same timeframe of test_data but was not included\n",
    "              pbar       = Progress Bar object from tqdm, to provide updates to\n",
    "        returns: FLOAT of Mean Absolute Error value of potential exogenous location when included into model\n",
    "    '''\n",
    "    clone_train_data = copy.deepcopy(train_data)\n",
    "    clone_test_data = copy.deepcopy(test_data)\n",
    "    clone_exotrain = exotrain if exotrain is None else copy.deepcopy(exotrain)\n",
    "    clone_exotest = exotest if exotest is None else copy.deepcopy(exotest)\n",
    "    \n",
    "    pbar = pbar if pbar is not None else tqdm(total=len(test_data)) # initialize counter\n",
    "    \n",
    "    predictions = model_creation_pred_one_step(clone_train_data, clone_test_data, clone_exotrain, clone_exotest, pbar)\n",
    "    mae = mean_absolute_error(test_data, predictions)\n",
    "    return(mae)\n",
    "\n",
    "def find_exmae(exog):\n",
    "    ''' Standalone task method to find mae of a given exogenous variable.  \n",
    "        Intended to be used as the function for the process pool and handle memory synchronization\n",
    "        args: exog = exogenous location data to be evaluated as a potential associated location to model\n",
    "        returns: Dictionary of exmae with columns\n",
    "        #bettermae state is saved to json file and updated synchronously across all forked processes\n",
    "    '''\n",
    "    extr, extest = train_test_split(exog, test_size=0.2, shuffle=False)\n",
    "    co = tuple(exog.columns)\n",
    "    exog_name = ','.join(co)\n",
    "    \n",
    "    shaObj = hashlib.sha1( bytes(exog.to_csv(), 'utf-8') )\n",
    "    data_signature = shaObj.hexdigest()\n",
    "    \n",
    "    # process syncrhonization on file read\n",
    "    lock.acquire()\n",
    "    try:\n",
    "        with open(results_filename, 'r') as all_results_file:\n",
    "            all_results = json.loads(all_results_file.read())\n",
    "    except:\n",
    "        all_results = { keymae['city']: {'exogen':{}} }\n",
    "    finally:\n",
    "        lock.release()\n",
    "    \n",
    "    exog_dict = all_results[keymae['city']]['exogen']\n",
    "    if exog_name in exog_dict and data_signature == exog_dict[exog_name]['data_source_sha1']:\n",
    "        exmae = exog_dict[exog_name]['exmae']\n",
    "        return { \"co\": co, \"exmae\": exmae }\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    exmae = maeFinder(tr, test, extr, extest, pbar)\n",
    "    \n",
    "    def save_solved_exmae(all_solutions, targetloc, exogloc, exmae, data_hash):\n",
    "        ''' handler function for adjustment of JSON relating to results_file, \n",
    "            see adjustfn for update_JSON_file()\n",
    "            args: all_solutions = loaded python-equivalent of json from file\n",
    "                  targetloc = keyword of target location of current keymae\n",
    "                  exogloc = exogenous location name that improves the model\n",
    "                  exmae = value of mean absolute value\n",
    "                  data_hash = sha1 digest of exog data set used to calculate exmae\n",
    "            returns: dictionary object \n",
    "        '''\n",
    "        all_solutions[targetloc]['exogen'][exogloc] = { 'exmae': exmae, 'data_source_sha1': data_hash }\n",
    "        return(all_solutions)\n",
    "    \n",
    "    lock.acquire()\n",
    "    try:\n",
    "        # Update status file with solved exmae\n",
    "        update_JSON_file(results_filename, save_solved_exmae, (keymae['city'], exog_name, exmae, data_signature))\n",
    "        \n",
    "        # Update bettermae array based on solved exmae if exmae is better than keymae\n",
    "        if exmae < keymae['mae']:\n",
    "            tmp_filename = tmp_bettermae_filename\n",
    "            update_JSON_file(tmp_filename, None, (exog_name, exmae))          # Save with default adjuster\n",
    "            \n",
    "    finally:\n",
    "        lock.release()\n",
    "        \n",
    "    return { \"co\": co, \"exmae\": exmae }\n",
    "\n",
    "\n",
    "def initExmaeWorker(l, kmae, train, testing, list_exoloc, progress_bar):\n",
    "    ''' Constructor function for creating and establishing initial/global \n",
    "        variables across process pool.\n",
    "        args: l = synchronization lock object\n",
    "              kmae = global keymae value\n",
    "              train = training dataframe object to use across processes\n",
    "              testing = testing dataframe object to use across processes\n",
    "              list_exoloc = list of exogenous locations related to target location\n",
    "              progress_bar = tqdm object for visual progress updates\n",
    "    '''\n",
    "    global lock\n",
    "    global keymae\n",
    "    global tr\n",
    "    global test\n",
    "    global l_exoloc\n",
    "    global pbar\n",
    "    lock = l\n",
    "    keymae = kmae\n",
    "    tr = train\n",
    "    test = testing\n",
    "    l_exoloc = list_exoloc\n",
    "    pbar = progress_bar\n",
    "\n",
    "\n",
    "def update_JSON_file(filename, adjustfn, arglist=(), kwargs={}, sort=True):\n",
    "    ''' Generic function to handle JSON file updates.  Reads-in entire file, \n",
    "        federates out updates with adjustment fn's, and then overwrites original file completely\n",
    "        Handles FileNotFoundError & JSONDecodeError automatically.\n",
    "        args: filename = json-encoded file on disk\n",
    "              adjustfn = function to perform adjustments to loaded dictionary file\n",
    "              arglist = positional args to pass on to adjustfn\n",
    "              kwargs = keyword args to pass on to adjustfn\n",
    "              sort = flag to auto-sort keys when saving to file [Default = True]\n",
    "        returns: dictionary object that was updated and saved to file\n",
    "    '''\n",
    "    def default_dict_adjustfn(data, key, value):\n",
    "        ''' Generic default function for updating a basic dictionary data file (top level keys only)\n",
    "            args: data = dictionary representation of JSON data from file\n",
    "                  key = key name to enter into dictionary\n",
    "                  value = value to enter into dictionary[key]\n",
    "            returns: Updated dictionary with key/value added\n",
    "        '''\n",
    "        data[key] = value\n",
    "        return(data)\n",
    "    \n",
    "    def default_list_adjustfn(data, value):\n",
    "        ''' Generic default function for updating a basic list data file (add to bottom of list)\n",
    "            args: data = list representation of JSON data from file\n",
    "                  value = value to append to end of list, list[len(list)] = value\n",
    "            returns: Updated list with value appended\n",
    "        '''\n",
    "        data.append(value)\n",
    "        return(data)\n",
    "    \n",
    "    loaded = False\n",
    "    while not loaded:\n",
    "        try:\n",
    "            file = open(filename, \"r+\")\n",
    "            json_data = json.loads(file.read())\n",
    "        except FileNotFoundError:\n",
    "            open(filename, \"w+\").close()       # create file on disk\n",
    "            continue\n",
    "        except json.JSONDecodeError:\n",
    "            json_data = {}\n",
    "        \n",
    "        loaded = True\n",
    "        if adjustfn is not None:\n",
    "            json_data = adjustfn(json_data, *arglist, **kwargs)\n",
    "        else:\n",
    "            if isinstance(json_data, dict):\n",
    "                json_data = default_dict_adjustfn(json_data, *arglist, **kwargs)\n",
    "            elif isinstance(json_data, list):\n",
    "                json_data = default_list_adjustfn(json_data, *arglist, **kwargs)\n",
    "            else:\n",
    "                raise ValueError('Unable to adjust JSON since function not provided or file not of type dict or list!')\n",
    "        \n",
    "        file.seek(0)                           # Go to first line, first column of file\n",
    "        file.write( json.dumps(json_data, sort_keys=sort, indent=4) )\n",
    "        file.truncate()                        # end file here, delete anything after the current file position\n",
    "        file.close()\n",
    "    \n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_filename = os.path.join(destdir,\"allMAE.json\")\n",
    "bettermae_results_filename = os.path.join(destdir,\"allBetterMAE.json\")\n",
    "tmp_bettermae_filename = os.path.join(destdir, \"tmp_bettermae.json\")\n",
    "\n",
    "def exogenous_var(data, ncloc, l_exoloc):\n",
    "    ''' Function to evaluate an location model completely.  First, it finds\n",
    "        a keymae of the current data frame about a location with 20% data split.\n",
    "        Secondly, it spawns a pool of processes (# of CPU cores) to calculate each potential\n",
    "        exogenous location's potential improvement of the model.  Each exmae is printed to\n",
    "        stdout and if improved, it is stored into the bettermae dictionary.  The NC location\n",
    "        does not complete until all exmaes have been calculated.\n",
    "        args: data  = entire dataframe of locations and rainfall amounts over time\n",
    "              ncloc = Name of NC location (matches column in data)\n",
    "              l_exoloc = list of exogenous locations to the ncloc parameter\n",
    "    '''\n",
    "    dat = data[ncloc]\n",
    "    tr, test = train_test_split(dat, test_size=0.2, shuffle=False)\n",
    "    keymae = { 'city': ncloc }\n",
    "    \n",
    "    shaObj = hashlib.sha1( bytes(dat.to_csv(), 'utf-8') )\n",
    "    data_signature = shaObj.hexdigest()\n",
    "    \n",
    "    try:\n",
    "        with open(results_filename, 'r') as all_results_file:\n",
    "            all_results = json.loads(all_results_file.read())\n",
    "    except:\n",
    "        all_results = {}\n",
    "    \n",
    "    if ncloc in all_results and data_signature == all_results[ncloc]['data_source_sha1']:\n",
    "        keymae['mae'] = all_results[ncloc]['keymae']\n",
    "    else:\n",
    "        keymae['mae'] = maeFinder(tr, test)\n",
    "        \n",
    "        # Save calculation to file\n",
    "        city_data = {'keymae':keymae['mae'],'data_source_sha1': data_signature,'exogen':{}}\n",
    "        update_JSON_file(results_filename, None, (keymae['city'], city_data) ) # save with default adjuster\n",
    "        \n",
    "        # wipe bettermae file of any keymae results since keymae has been recalculated\n",
    "        def delete_location(data, location):\n",
    "            if location in data:\n",
    "                del data[location]\n",
    "            return(data)\n",
    "            \n",
    "        update_JSON_file(bettermae_results_filename, delete_location, (keymae['city'],))\n",
    "        \n",
    "    # process exmaes\n",
    "    print('keymae of: '+ ncloc +' = '+str(keymae['mae']))\n",
    "    \n",
    "    poolLock = multiprocessing.Lock()\n",
    "    \n",
    "    def on_success(result):\n",
    "        print('exmae = {}'.format(result[\"co\"]) + ' '+ str(result[\"exmae\"]))\n",
    "        progressbar.update() # update counter of completion\n",
    "    \n",
    "    def on_error(err):\n",
    "        print(\"ERROR: {}\".format(err))\n",
    "        traceback.print_exception(type(err), err, err.__traceback__)\n",
    "    \n",
    "    process_limit = multiprocessing.cpu_count()-1   # 1 cpu is needed for basic OS functions\n",
    "    progressbar = tqdm(total=len(l_exoloc))  # initialize counter (regular)\n",
    "#     progressbar = tqdm(total=(len(l_exoloc)*len(test))) # initialize counter (multiple exmaes at once, fails do to process collision)\n",
    "    pool = multiprocessing.Pool(processes=process_limit, initializer=initExmaeWorker, initargs=(poolLock, keymae, tr, test, l_exoloc, progressbar))\n",
    "    for exog in l_exoloc:\n",
    "        pool.apply_async(find_exmae, args=(exog,), kwds={}, callback=on_success, error_callback=on_error)\n",
    "    \n",
    "    pool.close()      # no more tasks can be added for the pool to accomplish\n",
    "    pool.join()       # tell parent to wait until all tasks are accomplished by the process pool\n",
    "    \n",
    "    \n",
    "    # Evaluate & save found bettermae\n",
    "    if os.path.isfile(tmp_bettermae_filename):\n",
    "        tmp_bettermae_file = open(tmp_bettermae_filename, 'r')\n",
    "        improvement_exog = json.loads(tmp_bettermae_file.read())\n",
    "        tmp_bettermae_file.close()\n",
    "        os.remove(tmp_bettermae_file.name)                               # tmp file cleanup\n",
    "        \n",
    "        all_results_file = open(results_filename, 'r')\n",
    "        all_results = json.loads(all_results_file.read())\n",
    "        all_results_file.close()\n",
    "            \n",
    "        filtered_results = all_results[keymae['city']]\n",
    "        filtered_results['exogen'] = {}                              # reset dictionary\n",
    "        for key,value in improvement_exog.items():                   # fill exogen dictionary with valuable vars\n",
    "            filtered_results['exogen'][key] = value\n",
    "            \n",
    "        all_bettermae = update_JSON_file(bettermae_results_filename, None, (keymae['city'], filtered_results)) # save with default adjuster\n",
    "        print(\"Improvement_exog: \"+keymae['city']+\": {}\".format(json.dumps(all_bettermae[keymae['city']], indent=4)))\n",
    "    \n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T20:23:59.034914Z",
     "start_time": "2019-07-31T20:23:59.003666Z"
    }
   },
   "outputs": [],
   "source": [
    "def exog_combinations(df, exoe):\n",
    "    ''' This function takes the dataframe of rain data and the list of exogenous variables from a single NC\n",
    "    location and then returns a list of dataframes that contains all of the rainfall data for just the \n",
    "    exogenous variables\n",
    "    '''\n",
    "    lo_dfs = []\n",
    "    if len(exoe) == 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "    if len(exoe) > 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "        for ex in exoe:\n",
    "            lo_dfs.append(df.loc[:,[ex]])\n",
    "        if len(exoe) >2:\n",
    "            for i in range(2, len(exoe)):\n",
    "                combolist = list(combinations(exoe,i))\n",
    "                for c in combolist:\n",
    "                    lo_dfs.append(df.loc[:,c])\n",
    "    return(lo_dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Evaluation\n",
    "Finds combinations of exogenous variable locations and starts model evaluation of combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoExogen = True   # flag for manual use\n",
    "\n",
    "# Defining set of cities to evaluate\n",
    "if autoExogen or getpass.getuser() == \"rainfalld\":       # docker daemon, automatically do all exogen\n",
    "    todokeys = exogen.keys()\n",
    "else:    # manual setting of dictionary elements to do\n",
    "    todokeys = ('ARCOLA, NC', 'HENDERSON 2 NNW, NC', 'LAURINBURG, NC', 'ROANOKE RAPIDS, NC', 'MURFREESBORO, NC', 'LUMBERTON AREA, NC', 'LONGWOOD, NC', 'WHITEVILLE 7 NW, NC', 'CHARLOTTE AREA, NC', 'MOUNT MITCHELL AREA, NC', 'ASHEVILLE AIRPORT, NC', 'BANNER ELK, NC', 'BEECH MOUNTAIN, NC', 'BRYSON CITY 4, NC', 'BREVARD, NC', 'CASAR, NC', 'COWEETA EXP STATION, NC', 'CULLOWHEE, NC', 'FOREST CITY 8 W, NC', 'FRANKLIN, NC', 'GASTONIA, NC', 'GRANDFATHER MTN, NC', 'HENDERSONVILLE 1 NE, NC', 'HIGHLANDS, NC', 'HOT SPRINGS, NC', 'LAKE LURE 2, NC', 'LAKE TOXAWAY 2 SW, NC', 'MARSHALL, NC', 'MONROE 2 SE, NC', 'MOUNT HOLLY 4 NE, NC', 'OCONALUFTEE, NC', 'PISGAH FOREST 3 NE, NC', 'ROBBINSVILLE AG 5 NE, NC', 'ROSMAN, NC', 'SHELBY 2 NW, NC', 'TAPOCO, NC', 'TRYON, NC', 'WAYNESVILLE 1 E, NC', 'BOONE 1 SE, NC', 'DANBURY, NC', 'EDEN, NC', 'MOUNT AIRY 2 W, NC', 'REIDSVILLE 2 NW, NC', 'HAYESVILLE 1 NE, NC', 'MURPHY 4ESE, NC', 'KING, NC')\n",
    "\n",
    "sub_exogen = {k: exogen[k] for k in todokeys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T20:24:01.912170Z",
     "start_time": "2019-07-31T20:24:01.380989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaaa5351ee1b49e98071e94cde91e7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=43), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "l_o_dfs = defaultdict(list)\n",
    "for key,value in tqdm(sub_exogen.items()):\n",
    "    lo_dfs2 = exog_combinations(rd, value)\n",
    "    l_o_dfs[key] = lo_dfs2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_comb = [[4,3,3,4]]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files=[tmp_bettermae_filename]\n",
    "while (len(files) > 0):                          # reset results on new run\n",
    "    try:\n",
    "        os.remove( files[-1] )\n",
    "    except FileNotFoundError:                    # ignore since non-exist is the desired state\n",
    "        pass\n",
    "    except OSError as err:\n",
    "        traceback.print_exception(type(err), err, err.__traceback__)\n",
    "    finally:\n",
    "        files.pop()\n",
    "    \n",
    "for key,value in tqdm(l_o_dfs.items()):\n",
    "    exogenous_var(rd, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for the next 50 years (2019 - 2069)\n",
    "Includes exogenous locations outside of North Carolina to improve model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_exogs = ['WHITEVILLE 7 NW, NC', 'CASAR, NC', 'FOREST CITY 8 W, NC', 'GASTONIA, NC', 'LAKE LURE 2, NC', \n",
    "                       'ELIZABETHTOWN, NC', ' MOUNT HOLLY 4 NE, NC','GRANDFATHER MTN, NC']\n",
    "ncrd2 = ncrd.copy()\n",
    "ncrd_less = ncrd2.drop(with_exogs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_fx(data, begin, end):\n",
    "    base = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    date_list = [base + relativedelta(months=x) for x in range(600)]\n",
    "    prediction1_df = pd.DataFrame(index=date_list)\n",
    "    for col in tqdm(data.columns):\n",
    "        loc = data[col]\n",
    "        mod_fit1 = sarima_model_creation(loc, 4,0,3,3,0,4,12)\n",
    "        point_predictions = pd.DataFrame(mod_fit1.predict(start=begin, end=end), columns=[col])\n",
    "        future_pred1 = mod_fit1.get_prediction(start=begin, end=end)\n",
    "        future_pred1_ci = future_pred1.conf_int(alpha=0.5)\n",
    "        point_predictions_df = pd.merge(point_predictions, future_pred1_ci, left_index=True, right_index=True)\n",
    "        prediction1_df = pd.merge(prediction1_df, point_predictions_df, left_index=True, right_index=True)\n",
    "    return(prediction1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = prediction_fx(ncrd_less, '2019-05-01', '2069-05-01')\n",
    "pre_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_var_dict2 = {\n",
    "    'WHITEVILLE 7 NW, NC': rd[[' LORIS 2 S, SC']],\n",
    "    'CASAR, NC': rd[['GAFFNEY 6 E, SC']],\n",
    "    'FOREST CITY 8 W, NC': rd[['GAFFNEY 6 E, SC']],\n",
    "    'GASTONIA, NC': rd[['FORT MILL 4 NW, SC','GAFFNEY 6 E, SC']],\n",
    "    'LAKE LURE 2, NC': rd[['CHESNEE 7 WSW, SC']],\n",
    "    ' MOUNT HOLLY 4 NE, NC': rd[['CHESNEE 7 WSW, SC','GAFFNEY 6 E, SC']],\n",
    "    'ELIZABETHTOWN, NC': rd[[' LORIS 2 S, SC']],\n",
    "    'GRANDFATHER MTN, NC': rd[['ELIZABETHTON, TN']]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_exog_fx2(data, exog_dict, begin, end):\n",
    "    base = datetime.strptime(begin,'%Y-%m-%d')\n",
    "    date_list = [base + relativedelta(months=x) for x in range(600)]\n",
    "    prediction_df = pd.DataFrame(index = date_list)\n",
    "    pred_val_df = pd.DataFrame(index = date_list)\n",
    "    exog_predictions_df = pd.DataFrame(index = date_list)\n",
    "    for key,value in tqdm(exog_dict.items()):\n",
    "        loc = data[key]\n",
    "        mod_fit1 = sarima_model_creation(loc, 4,0,3,3,0,4, 12,exog=value)\n",
    "        if value.shape[1] > 1:\n",
    "            shap = value.shape[1]\n",
    "            for i in range(shap):\n",
    "                exog_mod_fit = sarima_model_creation(value.iloc[:,i],4,0,3,3,0,4,12)\n",
    "                e_preds2 = pd.DataFrame(exog_mod_fit.predict(start=begin, end=end))\n",
    "                if i is 0:\n",
    "                    exog_predictions_df = e_preds2\n",
    "                else:\n",
    "                    exog_predictions_df = pd.merge(exog_predictions_df, e_preds2, left_index=True, \n",
    "                                                   right_index=True)\n",
    "        else:\n",
    "            exog_mod_fit = sarima_model_creation(value, 4,0,3,3,0,4,12)\n",
    "            exog_predictions_df = pd.DataFrame(exog_mod_fit.predict(start=begin, end=end))\n",
    "        future_pred = mod_fit1.get_prediction(exog=exog_predictions_df,start=begin, end=end)\n",
    "        future_pred_ci = future_pred.conf_int(alpha=0.5)\n",
    "        future_pred_val= pd.DataFrame(mod_fit1.predict(exog=exog_predictions_df, start=begin, end=end), \n",
    "                                      columns = [key])\n",
    "        future_pred_full = pd.merge(future_pred_val, future_pred_ci, left_index=True, right_index=True)\n",
    "        prediction_df = pd.merge(prediction_df, future_pred_full, left_index=True, right_index=True)\n",
    "    return(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_ci_df = prediction_exog_fx2(rd, exo_var_dict2, '2019-05-01', '2069-05-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_ci_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals = pd.merge(pre_df, e_ci_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals.to_csv(os.path.join(destdir,'predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ci_vals.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
