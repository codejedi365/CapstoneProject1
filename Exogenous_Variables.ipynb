{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "file = 'rainfalldata.csv'\n",
    "rd = pd.read_csv(file)\n",
    "file2 = 'ncrainfalldata.csv'\n",
    "ncrd = pd.read_csv(file2)\n",
    "rd.Date = pd.to_datetime(rd.Date)\n",
    "rd = rd.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Month\n",
      "0  1887      1\n",
      "1  1888      1\n",
      "2  1889      1\n",
      "3  1890      1\n",
      "4  1891      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.full.Data_Wrangling_CAP1:161: RuntimeWarning: Mean of empty slice\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 472 entries, 1-1980 to 4-2019\n",
      "Columns: 235 entries, Raleigh, NC to row_number\n",
      "dtypes: float64(234), int64(1)\n",
      "memory usage: 890.2+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipynb.fs.full.Data_Wrangling_CAP1:264: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.Data_Wrangling_CAP1 import exofind\n",
    "latlong = pd.read_csv('latlong.csv')\n",
    "latlongsplit = latlong.iloc[0].apply(str.split, sep=',')\n",
    "latlongdf = pd.DataFrame(latlongsplit)\n",
    "latlongdf = latlongdf.drop(['Unnamed: 0','Raleigh AP, NC', 'Greensboro, NC', ' WILMINGTON 7 N, NC','LUMBERTON, NC','MYRTLE BEACH, SC','CHARLOTTE DOUGLAS AIRPORT, NC','GRNVL SPART INTL AP, SC','PICKENS, SC',' MT. MITCHELL, NC',' Caesars Head Area, SC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't work you can comment out this cell and then run the next cell.\n",
    "locations = rd.columns\n",
    "ncloc = locations[locations.str.endswith('NC')]\n",
    "valoc = locations[locations.str.endswith('VA')]\n",
    "scloc = locations[locations.str.endswith('SC')]\n",
    "galoc = locations[locations.str.endswith('GA')]\n",
    "tnloc = locations[locations.str.endswith('TN')]\n",
    "exoloc = valoc.append(galoc)\n",
    "exoloc = exoloc.append(scloc)\n",
    "exoloc = exoloc.append(tnloc)\n",
    "exogen = exofind(latlongdf, ncloc, exoloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes the stored exogen dictionary that is stored in the Data_Wrangling_CAP1 jupyter notebook\n",
    "# that was imported above.\n",
    "\n",
    "# %store -r exogen\n",
    "# exogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_model_creation(data, p, d, q, P, D, Q, m, exog=None):\n",
    "    my_order = [p,d,q]\n",
    "    my_sorder = [P,D,Q,m]\n",
    "    sarimamod = sm.tsa.statespace.SARIMAX(data, exog, order=my_order, seasonal_order=my_sorder, \n",
    "                                          enforce_stationarity=False, enforce_invertibility=False,\n",
    "                                          initialization='approximate_diffuse')\n",
    "    model_fit = sarimamod.fit()# start_params=[0, 0, 0, 0, 1])\n",
    "    return(model_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_hyper(it):\n",
    "    outlist = []\n",
    "    for AR in range(it):\n",
    "        for MA in range(it):\n",
    "            for SAR in range(it):\n",
    "                for SMA in range(it):\n",
    "                    outlist.append([AR,MA,SAR,SMA])\n",
    "    return(outlist)\n",
    "        \n",
    "config = iteration_hyper(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_find(training_data, comb, testing_data, search = False, exogtr = None, exogtest = None):\n",
    "    leastmae = 1000\n",
    "    for com in comb:\n",
    "        li_one_step = []\n",
    "        for i in range(len(testing_data)):\n",
    "            if i == 0:\n",
    "                copytraining = training_data.copy()\n",
    "                if exogtr is not None:\n",
    "                    excopy = exogtr.copy()\n",
    "                    mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, \n",
    "                                                  com[3], 12, exog=excopy)\n",
    "                    one_step_pred = mod_1.forecast(exog=excopy.iloc[[-12]]) #uses the data from the year before\n",
    "                    excopy = pd.concat([excopy, exogtest.iloc[[i]]])\n",
    "                else:\n",
    "                    mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, com[3], 12)\n",
    "                    one_step_pred = mod_1.forecast()\n",
    "                li_one_step.append(one_step_pred[0])\n",
    "                copytraining = pd.concat([copytraining, testing_data[[i]]])\n",
    "            else:\n",
    "                if exogtr is not None:\n",
    "                    mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, \n",
    "                                                  com[3], 12, exog=excopy)\n",
    "                    one_step_pred2 = mod_1.forecast(exog=excopy.iloc[[-12]])\n",
    "                    excopy = pd.concat([excopy, exogtest.iloc[[i]]])\n",
    "                else:\n",
    "                    mod_1 = sarima_model_creation(copytraining, com[0], 0, com[1], com[2], 0, com[3], 12)\n",
    "                    one_step_pred2 = mod_1.forecast()\n",
    "                li_one_step.append(one_step_pred2[0])\n",
    "                copytraining = pd.concat([copytraining, testing_data[[i]]])\n",
    "        mae = mean_absolute_error(testing_data, li_one_step)\n",
    "        if search is True:\n",
    "            if mae < leastmae:\n",
    "                leastmae = mae\n",
    "                H_AR = com[0]\n",
    "                H_MA = com[1]\n",
    "                H_SAR = com[2]\n",
    "                H_SMA = com[3]\n",
    "            print(com,mae)            \n",
    "    if search is True:\n",
    "        return('AR: '+ str(H_AR), 'MA: ' +str(H_MA), 'SAR: '+str(H_SAR), 'SMA: '+str(H_SMA))\n",
    "    else:\n",
    "        return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exog_combinations(df, exoe):\n",
    "    lo_dfs = []\n",
    "    if len(exoe) == 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "    if len(exoe) > 1:\n",
    "        lo_dfs.append(df.loc[:,exoe])\n",
    "        for ex in exoe:\n",
    "            lo_dfs.append(df.loc[:,[ex]])\n",
    "        if len(exoe) >2:\n",
    "            for i in range(2, len(exoe)):\n",
    "                combolist = list(combinations(exoe,i))\n",
    "                for c in combolist:\n",
    "                    lo_dfs.append(df.loc[:,c])\n",
    "    return(lo_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exogenous_var(data, exo_dict, best_comb):\n",
    "    for key, value in exo_dict.items():\n",
    "        dat = data[key]\n",
    "        l_exog = exog_combinations(data, value)\n",
    "        tr, test = train_test_split(dat, test_size = 0.2, shuffle=False)\n",
    "        keymae = hyperparameter_find(tr, best_comb, test)\n",
    "        print('keymae of: '+ key +' = '+str(keymae))\n",
    "        bettermae = {}\n",
    "        for exog in l_exog:\n",
    "            extr, extest = train_test_split(exog, test_size = 0.2, shuffle=False)\n",
    "            exmae = hyperparameter_find(tr, best_comb, test, exogtr=extr, exogtest = extest)\n",
    "            co = tuple(exog.columns)\n",
    "            print('exmae = {}'.format(co) + ' '+ str(exmae))\n",
    "            if exmae < keymae:\n",
    "                bettermae[co] = exmae\n",
    "                bettermae2 = {key: bettermae}\n",
    "    return(bettermae2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_comb = [[4,3,3,4]]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "exogenous_var(rd, exogen, best_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to pause the code from running and then start back up from where you left off you can use the\n",
    "# following code to get a sub_dictionary that will be only a portion of the large dictionary (exogen) that I am\n",
    "# using\n",
    "\n",
    "# I had to do this because my computer started running really slow so I had to restart it in order to let the \n",
    "# memory reset and so it could start running at normal speed again.\n",
    "\n",
    "# use exogen.keys() to list all of the keys then find the location where you left off from the printout above\n",
    "# and then copy and paste the rest of the keys into the \"todokeys\" tuple below. Then call sub_exogen in place\n",
    "# of exogen in the \"exogenous_var\" function above. \n",
    "\n",
    "\n",
    "# todokeys = ('Henderson 2 NNW, NC', 'JACKSON SPRINGS 5 WNW, NC', 'Laurinburg, NC', 'Louisburg, NC', 'Roanoke Rapids, NC', 'Rougemont, NC', 'Murfreesboro, NC', 'Lumberton Area, NC', 'ELIZABETHTOWN, NC', 'LONGWOOD, NC', 'WHITEVILLE 7 NW, NC', ' WILLIAM O HUSKE L&D, NC', 'Asheville Area, NC', 'Charlotte Area, NC', 'Mount Mitchell Area, NC', 'ASHEVILLE AIRPORT, NC', 'BANNER ELK, NC', 'BEECH MOUNTAIN, NC', 'BRYSON CITY 4, NC', 'BLACK MOUNTAIN 2 W, NC', 'BREVARD, NC', 'BRIDGEWATER HYDRO, NC', 'CASAR, NC', 'CATAWBA 3 NNW, NC', 'CONCORD, NC', 'COWEETA EXP STATION, NC', 'CULLOWHEE, NC', 'FOREST CITY 8 W, NC', 'FRANKLIN, NC', 'GASTONIA, NC', 'GRANDFATHER MTN, NC', ' HENDERSONVILLE 1 NE, NC', 'Hickory FAA Airport, NC', ' HIGHLANDS, NC', 'HOT SPRINGS, NC', 'LAKE LURE 2, NC', 'LAKE TOXAWAY 2 SW, NC', 'LENOIR, NC', 'LINCOLNTON 4 W, NC', 'MARION, NC', 'MARSHALL, NC', 'MONROE 2 SE, NC', 'MORGANTON, NC')\n",
    "# sub_exogen = {k: exogen[k] for k in todokeys}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
